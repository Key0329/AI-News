# Feature Specification: AI & AI Coding 自動化情報助手

**Feature Branch**: `001-ai-news-assistant`
**Created**: 2026-01-05
**Status**: Draft
**Input**: User description: "建立一個「AI & AI Coding 自動化情報助手」

## 目標與願景
身為一名開發者，我希望系統能定時自動蒐集全球最新的 AI 與 AI coding 相關資訊（新聞、推文、專案），並整理成易於閱讀的格式，讓我能高效掌握技術趨勢而不必手動搜尋。"

## Clarifications

### Session 2026-01-05

- Q: 資料持久化策略 - 系統產生的摘要報告和蒐集到的資訊項目是否需要持久化儲存？ → A: 暫存當日資料供檢查和重試，隔日自動清理
- Q: 外部來源錯誤處理 - 當系統遇到資訊來源無法連線、解析失敗或速率限制時，應該如何處理？ → A: 記錄錯誤並跳過該來源，繼續處理其他來源
- Q: 內容摘要生成機制 - 系統應該使用什麼方法來產生摘要和翻譯？ → A: 使用付費 AI API（如 OpenAI、Anthropic）進行摘要和翻譯
- Q: 系統部署環境 - 系統應該部署在什麼環境中運行？ → A: 本地機器或自有伺服器上運行
- Q: API 金鑰與敏感資訊管理 - 系統應該如何儲存和管理 API 金鑰和認證資訊？ → A: 使用環境變數儲存
- Q: MVP 範圍調整 - 第一階段應該支援哪些資訊來源類型？ → A: 專注於 RSS 訂閱來源（主流 AI 新聞、官方部落格）和公開新聞/名人部落格，暫時移除 Twitter 和 GitHub 蒐集需求
- Q: 具體資訊來源與技術方向 - 應該使用哪些具體來源，以及採用什麼技術策略？ → A: 採用三層級來源架構（官方部落格、技術新聞聚合 API、深度技術電子報、開發者社群），優先使用 API 來源（NewsAPI、Hacker News API、Reddit API），其次才是 RSS
- Q: 資料來源配置化 - 如何管理資訊來源配置和敏感資訊？ → A: 採用配置檔案（如 JSON/YAML）管理所有來源（RSS URL、API 端點、關鍵字等），敏感憑證（API Keys）透過環境變數注入，系統採用模組化設計支援未來擴展新管道

### Session 2026-01-06

- Q: 系統應該使用什麼具體的去重演算法來識別跨來源的重複內容？ → A: 使用相對相似度門檻（如標題 80% 相似度）結合內容指紋比對
- Q: 系統應該如何記錄和儲存執行日誌？ → A: 記錄完整的執行日誌到檔案，包含時間戳、來源狀態、錯誤詳情、成功/失敗摘要統計
- Q: 摘要報告的 Markdown 檔案應該採用什麼具體的結構和格式？ → A: 分層級結構（標題 → 日期 → 各層級來源區塊 → 每則資訊包含標題/摘要/元資料/連結）
- Q: 系統應該如何判斷內容是否與「AI 應用」及「AI 輔助程式碼開發」相關？ → A: 使用 AI API 進行語義判斷，檢查內容是否包含 AI 模型、AI 工具、程式碼輔助、開發框架等相關主題
- Q: 當推送摘要報告失敗時，系統應該採用什麼重試策略？ → A: 在下一次排程執行時重試一次，失敗則記錄並跳過

## User Scenarios & Testing *(mandatory)*

### User Story 1 - 自動定時蒐集與摘要產生 (Priority: P1)

作為使用者，我希望系統能在每天固定時間（例如早上 8:00）自動執行，蒐集過去 24 小時內來自至少 3 個不同來源的 AI 與 AI Coding 相關資訊，並產生一份包含至少 5 則資訊的繁體中文摘要報告。

**Why this priority**: 這是系統的核心價值所在。若無法自動蒐集並產生摘要，使用者仍需手動搜尋，失去了自動化的意義。此功能是 MVP 的基礎。

**Independent Test**: 可透過設定排程時間並等待系統執行，驗證是否能在指定時間自動產生包含多來源資訊的摘要報告。

**Acceptance Scenarios**:

1. **Given** 系統已設定每天早上 8:00 執行排程，**When** 時間到達 8:00，**Then** 系統應自動開始蒐集流程，並在執行完畢後產生一份摘要報告
2. **Given** 系統執行蒐集動作，**When** 蒐集完成，**Then** 摘要報告應包含至少 3 個不同 RSS 來源或新聞網站的資訊
3. **Given** 系統執行蒐集動作，**When** 蒐集完成，**Then** 摘要報告應包含至少 5 則獨立的資訊項目
4. **Given** 蒐集到的內容為英文，**When** 系統處理該內容，**Then** 產出的摘要必須使用繁體中文
5. **Given** 系統產生摘要報告，**When** 使用者檢視報告，**Then** 每則資訊應包含標題、3-5 點核心摘要、來源、發布時間及原始連結
6. **Given** 系統執行蒐集流程（無論成功或失敗），**When** 執行結束，**Then** 系統應將完整的執行日誌寫入檔案，包含時間戳、各來源蒐集狀態、錯誤詳情（若有）、成功/失敗摘要統計

---

### User Story 2 - 智能內容過濾與去重 (Priority: P2)

作為使用者，我希望系統能自動過濾出與「AI 應用」及「AI 輔助程式碼開發」高度相關的內容（例如大型語言模型更新、AI 程式碼工具如 Copilot 或 Claude Code、AI 代理架構 Agentic Workflow），並在多個來源提到同一則新聞時，只保留最完整的版本。

**Why this priority**: 內容品質直接影響使用者體驗。若摘要充斥無關資訊或重複內容，使用者仍需花時間篩選，降低系統價值。此功能提升資訊精準度。

**Independent Test**: 可透過提供包含相關與無關內容的測試資料，驗證系統是否能正確過濾並去除重複項目。

**Acceptance Scenarios**:

1. **Given** 系統從多個來源蒐集到包含「AI 輔助程式碼開發」、「大型語言模型」及「一般科技新聞」的混合內容，**When** 系統透過 AI API 進行語義判斷過濾，**Then** 產出的摘要應僅包含與 AI 模型、AI 工具、程式碼輔助、開發框架等相關主題的內容，一般科技新聞應被排除
2. **Given** 三個不同來源都報導了同一則關於 Claude Code 的更新，**When** 系統識別到重複內容（標題相似度達 80% 以上或內容指紋相同），**Then** 系統應僅保留一份，並優先選擇內容最完整的版本
3. **Given** 系統識別到重複內容，**When** 選擇保留哪一份，**Then** 系統應根據內容完整度（例如字數、包含的技術細節數量）進行判斷
4. **Given** 系統執行去重邏輯，**When** 比較兩則新聞標題，**Then** 系統應使用字串相似度演算法（如 Levenshtein 距離或 Cosine 相似度）計算相似度百分比，並在達到 80% 門檻時標記為重複
5. **Given** 系統過濾內容，**When** 遇到邊界案例（例如同時提到 AI 和其他科技主題的文章），**Then** 系統應根據 AI 相關性比重決定是否納入

---

### User Story 3 - 多層級資訊來源整合（MVP 範圍）(Priority: P3)

作為使用者，我希望系統能從三個層級的資訊來源蒐集 AI 與 AI Coding 相關資訊：

**層級 1 - 核心模型與 AI 實驗室（RSS 優先）**：
這些來源決定了 AI 能力的上限，每一則更新都可能改變 AI Coding 的底層邏輯。
- OpenAI Blog (https://openai.com/news/rss.xml) - GPT 系列模型更新與 API 變更
- Anthropic News (https://www.anthropic.com/news-rss.xml) - Claude 模型動態
- Google DeepMind (https://deepmind.google/blog/rss.xml) - Gemini 與 AI 前沿研究
- Meta AI (https://ai.meta.com/blog/rss/) - Llama 開源系列
- Mistral AI (https://mistral.ai/news/index.xml) - 歐洲最強模型，注重效能與成本平衡

**層級 2 - AI Coding 編輯器與工具（RSS + GitHub Release）**：
作為前端工程師，這些工具的 Release Notes 會直接改變工作流。
- Cursor Blog (https://www.cursor.com/blog/feed.xml) - AI 原生編輯器動態
- GitHub Changelog (https://github.blog/changelog/feed/) - GitHub Copilot 與 GitHub Spark 功能更新
- Codeium Blog (https://codeium.com/blog/rss.xml) - Windsurf 等 Coding Agent 更新
- VS Code Release (GitHub: microsoft/vscode) - 插件系統與 AI 核心 API 變動
- Zed Editor (GitHub: zed-industries/zed) - 高性能編輯器與 AI 多模型串接

**層級 3 - 開發框架、SDK 與社群（GitHub + RSS）**：
決定如何將 AI 整合進 Vue/JS 專案，以及掌握社群真實評測。
- Vercel AI SDK (GitHub: vercel/ai) - 前端整合 AI 最重要的 SDK
- MCP Servers (GitHub: modelcontextprotocol/servers) - 模型上下文協定擴充實作
- LangChain.js (GitHub: langchain-ai/langchainjs) - 建立複雜 AI Agent 邏輯
- LlamaIndex.ts (GitHub: run-llama/LlamaIndexTS) - 處理 RAG 的首選工具
- Hacker News AI (https://hnrss.org/frontpage?q=AI) - 全球頂尖工程師討論
- Reddit LocalLLaMA (https://www.reddit.com/r/LocalLLaMA/.rss) - 本地模型真實評測
- TLDR AI (https://tldr.tech/ai) - 每日 AI 摘要
- Ben's Bites (https://www.bensbites.co/) - 商業與工具類 AI 趨勢

**Why this priority**: 這份清單結合高質量 RSS 來源（適合穩定採集）與 GitHub Repos（適合追蹤 Release Notes），能確保從底層模型到前端工具的全方位動態掌握。官方部落格提供最權威的模型發佈資訊，編輯器工具更新直接影響開發體驗，開發框架決定實戰應用能力，社群提供真實評測與去重參考。

**Independent Test**: 可透過配置各層級的 API 金鑰和來源清單，驗證系統是否能成功從每個層級蒐集資訊。

**Acceptance Scenarios**:

1. **Given** 系統已配置層級 1 的 RSS 來源（如 OpenAI Blog、Anthropic News、DeepMind），**When** 執行蒐集動作，**Then** 系統應成功從至少 3 個官方部落格獲取最新文章，並擷取標題、摘要、發布時間及連結
2. **Given** 系統已配置層級 2 的 GitHub 來源（如 microsoft/vscode、zed-industries/zed），**When** 執行蒐集動作，**Then** 系統應透過 GitHub API 獲取最新 Release Notes，並擷取版本號、發布日期、更新內容及 Release 連結
3. **Given** 系統已配置層級 3 的混合來源（GitHub Repos 如 vercel/ai、RSS 如 Hacker News AI），**When** 執行蒐集動作，**Then** 系統應成功處理不同格式的資料（GitHub JSON、RSS XML），並統一轉換為內部資料結構
4. **Given** 系統從 RSS feed 蒐集文章（如 Cursor Blog、Meta AI），**When** 處理 RSS 2.0 或 Atom 格式資料，**Then** 系統應正確解析 XML 並擷取標題、描述、發布時間、作者（若有）及原始連結
5. **Given** 系統從 GitHub API 追蹤 Repository Release（如 langchain-ai/langchainjs），**When** 處理 GitHub API 回傳的 JSON，**Then** 系統應擷取 Release 標題、Tag 名稱、發布時間、Release Body（Markdown 格式）及 Release URL
6. **Given** 某個 RSS 來源或 GitHub API 暫時無法連線（網路錯誤或 API 限制），**When** 系統嘗試存取該來源，**Then** 系統不應崩潰，應跳過該來源並在日誌中記錄錯誤訊息、來源名稱、來源層級及錯誤類型
7. **Given** 系統從多個來源蒐集到相同的資訊（例如 Hacker News 和官方部落格都報導了 Claude 3.5 Sonnet 更新），**When** 系統執行去重邏輯，**Then** 系統應保留含有 GitHub 連結或官方部落格的版本，優先保留層級較高（官方）的來源

---

### User Story 4 - 報告推送與外部整合 (Priority: P4)

作為使用者，我希望產生的摘要報告能自動推送至外部通訊頻道（例如電子郵件、Slack、Discord），讓我無需主動查看即可接收最新資訊。

**Why this priority**: 主動推送提升便利性，但屬於使用者體驗增強。初期版本可先提供本地檔案輸出，使用者可手動查看，仍能滿足核心需求。

**Independent Test**: 可透過設定推送目標（例如測試用電子郵件地址），驗證系統是否能成功傳送摘要報告。

**Acceptance Scenarios**:

1. **Given** 系統已產生當日摘要報告，**When** 推送流程啟動，**Then** 系統應將報告傳送至使用者設定的外部通訊頻道（例如電子郵件）
2. **Given** 推送至電子郵件，**When** 使用者收到郵件，**Then** 郵件主旨應包含日期（例如「2026-01-05 AI Coding 情報摘要」），內容應為格式化的 HTML 或 Markdown
3. **Given** 使用者檢視摘要報告，**When** 開啟 Markdown 檔案，**Then** 報告應採用分層級結構，依序顯示：報告標題 → 日期 → 層級 1 資訊區塊 → 層級 2 資訊區塊 → 層級 3 資訊區塊，每則資訊包含標題、摘要、元資料及連結
4. **Given** 推送失敗（例如網路問題），**When** 系統偵測到錯誤，**Then** 系統應記錄錯誤日誌並標記該報告為「待重試」，在下次排程執行時自動重試一次
5. **Given** 推送失敗且已重試一次仍失敗，**When** 系統完成第二次推送嘗試，**Then** 系統應記錄錯誤並標記該報告為「推送失敗-已放棄」，不再重試，使用者可手動查看本地摘要檔案

---

### User Story 5 - 資料來源配置化管理 (Priority: P2)

作為使用者，我希望系統能從獨立的設定檔（如 JSON 或 YAML）讀取所有資訊來源配置（RSS URL、API 端點、搜尋關鍵字等），而所有敏感憑證（API Keys）透過環境變數注入，讓我能輕鬆新增或修改來源而無需改動程式碼。同時，系統應採用模組化架構，未來增加新的資訊管道（如 Podcast、Newsletter）時不需大幅改動核心邏輯。

**Why this priority**: 配置化設計符合「避免過度設計」的同時提供必要的彈性。使用者可透過編輯設定檔快速調整來源清單，而開發者可透過模組化架構輕鬆擴展新管道。這是高品質軟體的基本要求，也讓系統更易於維護和測試。

**Independent Test**: 可透過修改設定檔新增/移除來源，並驗證系統是否能正確載入配置；可透過模擬新管道測試擴展機制是否運作正常。

**Acceptance Scenarios**:

1. **Given** 系統啟動時，**When** 載入設定檔（如 `sources.json` 或 `sources.yaml`），**Then** 系統應成功解析所有來源配置，包括來源名稱、類型、層級、URL/端點、關鍵字等
2. **Given** 設定檔中定義了 5 個資訊來源（3 個 RSS、2 個 API），**When** 系統執行蒐集，**Then** 系統應嘗試從所有 5 個來源獲取資訊
3. **Given** 使用者在設定檔中新增一個新的 RSS 來源，**When** 重新啟動系統並執行蒐集，**Then** 新來源應被包含在蒐集流程中，無需修改程式碼
4. **Given** 設定檔中某個 API 來源需要 API 金鑰，**When** 系統讀取該來源配置，**Then** 系統應從對應的環境變數（如 `NEWSAPI_KEY`）讀取金鑰，而非從設定檔中讀取明文金鑰
5. **Given** 設定檔格式錯誤或缺少必要欄位，**When** 系統啟動時載入配置，**Then** 系統應顯示清楚的錯誤訊息，指出哪個來源配置有問題及缺少哪些欄位
6. **Given** 使用者想停用某個來源但不刪除配置，**When** 在設定檔中將該來源的 `enabled` 設為 `false`，**Then** 系統應跳過該來源的蒐集
7. **Given** 開發者想新增一個新的管道類型（如 Podcast RSS），**When** 實作新的來源處理模組並註冊到系統，**Then** 系統應能載入並處理該類型的來源，無需修改現有的蒐集器或過濾器邏輯

---

### Edge Cases

- 當系統識別到的相關內容少於 5 則時，應如何處理？（應放寬過濾條件或延長時間範圍至 48 小時，確保提供足夠資訊）
- 當用戶設定的排程時間與系統維護時間衝突時，應如何處理？（「系統維護時間」定義為使用者主動觸發的系統更新、配置檔案重大變更、或作業系統維護時段。若排程時間落在維護時段內，系統應在維護結束後立即執行蒐集任務，並在日誌中記錄延遲原因及實際執行時間）
- 當資料清理流程執行時系統正在產生新的摘要報告，應如何處理？（清理流程應僅刪除已超過保留期限的舊資料，不影響當日正在產生的資料）
- 當報告推送成功但接收端（郵件伺服器）延遲確認的場景，應如何處理？（系統應設定推送確認超時時間為 60 秒，超時後視為推送失敗並按重試策略處理）

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: 系統必須支援定時排程機制，允許使用者設定每日固定執行時間（例如每天早上 8:00）。排程執行時間精確度為分鐘級別（±1 分鐘），使用使用者系統的本地時區作為預設時區
- **FR-002**: 系統必須支援三層級資訊來源蒐集：層級 1（核心模型與 AI 實驗室的官方部落格 RSS，如 OpenAI、Anthropic、DeepMind、Meta AI、Mistral AI）、層級 2（AI Coding 編輯器與工具的 RSS 及 GitHub Release 追蹤，如 Cursor、GitHub Changelog、VS Code、Codeium、Zed Editor）、層級 3（開發框架 SDK 的 GitHub Release 及社群 RSS，如 Vercel AI SDK、MCP Servers、LangChain.js、LlamaIndex.ts、Hacker News AI、Reddit LocalLLaMA、TLDR AI、Ben's Bites）
- **FR-003**: 系統必須能識別內容是否與「AI 應用」及「AI 輔助程式碼開發」相關，透過 AI API 進行語義判斷，檢查內容是否包含 AI 模型（如 GPT、Claude、Llama）、AI 工具（如 Copilot、Cursor）、程式碼輔助、開發框架（如 LangChain、Vercel AI SDK）等相關主題，並過濾無關內容
- **FR-004**: 系統必須能識別來自不同來源的重複內容，使用相對相似度門檻（標題 80% 相似度）結合內容指紋比對，並僅保留內容最完整的版本。「內容最完整」的判斷標準依序為：(1) 字數較多（至少多 20%），(2) 包含技術細節數量較多（如版本號、API 名稱、程式碼範例），(3) 來源層級較高（層級 1 > 層級 2 > 層級 3）
- **FR-005**: 系統必須能透過外部 AI API（如 OpenAI 或 Anthropic）從英文內容中提煉 3-5 點核心摘要，並將摘要翻譯為繁體中文。「點」指的是 Markdown 條列項目（使用 `- ` 或 `* ` 開頭），每個條列項目為一個完整句子，長度建議在 15-40 字之間
- **FR-006**: 系統必須為每則資訊標註來源名稱、作者（若有）、發布時間及原始連結
- **FR-007**: 系統必須產生結構化的摘要報告，格式為 Markdown，採用分層級結構：報告標題 → 日期 → 各層級來源區塊（層級 1：核心模型與 AI 實驗室、層級 2：AI Coding 編輯器與工具、層級 3：開發框架 SDK 與社群）→ 每則資訊包含標題、3-5 點核心摘要、來源名稱、作者（若有）、發布時間及原始連結
- **FR-008**: 系統必須在蒐集流程中遇到來源無法連線或 API 速率限制時，跳過該來源並記錄錯誤日誌（包含錯誤類型和時間），不影響其他來源的蒐集
- **FR-009**: 系統必須支援推送摘要報告至至少一種外部通訊頻道（例如電子郵件），當推送失敗時，系統應在下一次排程執行時自動重試一次，若重試後仍失敗則記錄錯誤並標記為「推送失敗-已放棄」，不再重試
- **FR-010**: 系統必須將每次執行的完整日誌記錄到檔案，包含時間戳、各來源的蒐集狀態、錯誤詳情（錯誤類型、錯誤訊息、發生時間）、蒐集到的資訊數量、執行時間，以及成功/失敗的摘要統計
- **FR-011**: 系統必須暫存當日蒐集的資訊項目和產生的摘要報告，供檢查和重試使用，並在隔日自動清理前一日的資料
- **FR-012**: 系統必須在 AI API 調用失敗或超時時，保留原始英文內容並在報告中標註「摘要生成失敗」，不中斷整體蒐集流程
- **FR-013**: 系統必須透過環境變數讀取所有敏感配置（包括 API 金鑰、認證資訊），不在程式碼或設定檔中硬編碼
- **FR-014**: 系統必須支援使用關鍵字（如「AI coding」、「large language model」、「GPT」、「Claude」等）查詢技術新聞聚合 API，並可配置關鍵字清單
- **FR-015**: 系統必須能處理來自不同來源的資料格式（GitHub API 的 JSON、RSS 2.0/Atom 的 XML、HTML from Web），並統一轉換為內部資料結構。特別支援 GitHub Release API 的 JSON 格式解析（包括 tag_name、published_at、body 等欄位）
- **FR-016**: 系統必須支援從外部配置檔案（JSON 或 YAML 格式）讀取所有資訊來源配置，包括來源名稱、類型、層級、URL/API 端點、搜尋關鍵字等非敏感資訊
- **FR-017**: 系統必須在啟動時驗證配置檔案的格式正確性和必要欄位完整性，若發現錯誤應顯示清楚的錯誤訊息並列出問題所在
- **FR-018**: 系統必須支援透過配置檔案中的 `enabled` 欄位啟用或停用特定資訊來源，停用的來源應被跳過而不影響其他來源的蒐集
- **FR-019**: 系統必須在每次排程執行時重新載入配置檔案，允許使用者修改來源配置而無需重啟系統
- **FR-020**: 系統必須採用模組化架構，支援註冊新的來源處理模組（如 RSS 處理器、API 處理器、Web 抓取器），新增管道類型時無需修改核心蒐集邏輯
- **FR-021**: 系統必須限制每個來源的最大抓取數量為 20 則資訊，當來源返回超過此數量時，應優先選擇熱度最高的資訊。「熱度」衡量指標依序為：(1) 社群平台的投票數或評分（如 Hacker News 投票數、Reddit upvotes），(2) 發布時間（較新的優先），(3) 評論或互動數量
- **FR-022**: 系統必須為配置檔案定義預設路徑為 `./config/sources.json` 或 `./config/sources.yaml`，並支援透過環境變數 `AI_NEWS_CONFIG_PATH` 自訂配置檔案路徑
- **FR-023**: 系統必須為摘要報告定義儲存路徑為 `./output/digests/YYYY-MM-DD-digest.md`（依日期命名），並支援透過環境變數 `AI_NEWS_OUTPUT_PATH` 自訂輸出目錄。產生的檔案應設定為使用者可讀寫權限（644）
- **FR-024**: 系統必須限制單次外部 API 調用（包括 AI API、NewsAPI、GitHub API、RSS 抓取）的超時時間為 30 秒，超時後應記錄錯誤並跳過該來源
- **FR-025**: 系統必須限制並發蒐集來源的最大數量為 5 個，避免資源耗盡。當來源數量超過此限制時，應分批處理
- **FR-026**: 系統必須限制單次 AI API 調用（摘要生成）的重試次數為 2 次，重試間隔為 5 秒。若 2 次重試後仍失敗，應保留原始英文內容並標註「摘要生成失敗」
- **FR-027**: 系統必須限制摘要報告 Markdown 檔案的最大大小為 5MB，超過此大小時應截斷較舊或較低優先級的資訊項目，並在報告開頭添加警告訊息
- **FR-028**: 系統必須在啟動時驗證以下環境變數是否存在：(1) AI API 金鑰（如 `OPENAI_API_KEY` 或 `ANTHROPIC_API_KEY`），(2) 外部 API 金鑰（如 `NEWSAPI_KEY`、`GITHUB_TOKEN`、`REDDIT_CLIENT_ID`），(3) 推送服務憑證（如 `EMAIL_SMTP_PASSWORD`）。若缺少關鍵環境變數，應顯示清楚的錯誤訊息並列出缺少的變數名稱，終止執行
- **FR-029**: 系統必須支援手動觸發執行機制，透過命令列參數（如 `--run-now` 或 `--manual`）立即執行蒐集流程，不受排程時間限制。此功能用於補救排程遺漏或測試用途
- **FR-030**: 系統必須在意外崩潰或手動重啟後，檢查是否有未完成的蒐集任務或未推送的報告。若偵測到未完成任務，應在日誌中記錄並提供選項讓使用者決定是否重新執行或跳過
- **FR-031**: 系統必須驗證蒐集資料的完整性，檢查每則資訊是否包含必要欄位（標題、來源、連結）。若缺少必要欄位，應記錄警告並使用預設值（如「標題未提供」、「來源未知」），但仍保留該則資訊
- **FR-032**: 系統必須在「發布時間未知」時顯示為「時間未提供」，並將該則資訊排序至該來源的最後位置
- **FR-033**: 系統必須強制使用 HTTPS 連線存取所有外部 API 和 RSS 來源，並驗證 SSL/TLS 憑證有效性。若遇到憑證驗證失敗，應記錄錯誤並跳過該來源
- **FR-034**: 系統必須在執行日誌中遮蔽敏感資訊，包括 API 金鑰（僅顯示前 4 碼和後 4 碼，中間以 `***` 取代）、密碼、認證 Token。完整憑證絕不應出現在日誌檔案中
- **FR-035**: 系統必須在第一次執行時（無歷史資料、無快取）初始化必要的目錄結構（如 `./config/`、`./output/digests/`、`./logs/`），並產生範例配置檔案供使用者參考
- **FR-036**: 系統必須在配置遷移或系統升級時，驗證配置檔案版本並提供向後相容性支援。若偵測到舊版配置格式，應顯示警告訊息並提供遷移指引
- **FR-037**: 系統必須在部分來源失敗時，評估降級服務品質。若成功蒐集的來源少於 2 個，應在報告中添加警告訊息說明資訊來源不足，但仍產生報告
- **FR-038**: 系統必須處理所有來源都返回 0 則有效資訊的情況，產生一份標註「今日無符合條件的資訊」的空報告，並記錄原因（如所有來源失敗、過濾後無相關內容等）
- **FR-039**: 系統必須限制單一來源返回的單篇文章內容大小為 1MB，超過此大小時應截斷內容至前 1MB 並添加「內容已截斷」標註
- **FR-040**: 系統必須處理 RSS feed 格式不符合標準的情況（如缺少 pubDate、title 等必要欄位），使用容錯機制提取可用資訊，並在日誌中記錄格式警告
- **FR-041**: 系統必須限制摘要報告包含的資訊總數上限為 100 則，超過此數量時應優先保留高熱度、高層級來源的資訊，並在報告開頭添加「已達資訊數量上限，部分低優先級資訊已省略」警告
- **FR-042**: 系統必須在配置檔案於系統執行過程中被修改時，於下次排程執行時重新載入配置，並在日誌中記錄「配置已重新載入」及變更的來源清單
- **FR-043**: 系統必須處理時區切換（如日光節約時間 DST 變更）對排程時間的影響，在時區變更後自動調整排程時間以維持相同的實際執行時間（例如永遠在本地時間 8:00 執行）
- **FR-044**: 系統必須處理同一層級內多個來源同時失敗的情況，在日誌中記錄該層級的整體失敗率，並在報告中標註該層級資訊來源不足

### Key Entities

- **資訊項目（News Item）**: 代表單一則 AI 或 AI Coding 相關資訊。包含屬性：標題、摘要（3-5 點核心內容）、來源名稱、來源層級（1/2/3）、來源類型（API/RSS/Web）、作者（若有）、發布時間、評分或投票數（若來自社群平台）、原始連結、語言（原始語言）、蒐集時間、保留截止日期（蒐集日期+1天）
- **資訊來源（Source）**: 代表系統蒐集資訊的來源。包含屬性：來源名稱、來源層級（1=官方部落格/2=新聞聚合 API/3=社群與電子報）、來源類型（API/RSS/Web）、存取方式（REST API/RSS feed/網頁抓取）、API 端點或 URL、需要 API 金鑰（是/否）、搜尋關鍵字（若適用）、是否啟用、上次成功蒐集時間、上次錯誤訊息、上次錯誤類型（連線失敗/API 限制/解析失敗/認證失敗/其他）
- **摘要報告（Daily Digest）**: 代表每日產生的整合報告。包含屬性：報告日期、包含的資訊項目清單（按層級分組）、總資訊數量、來源數量、各層級資訊數量（層級 1/2/3 各多少則）、報告檔案格式（Markdown）、報告結構類型（分層級結構）、產生時間、推送狀態（待推送/推送成功/推送失敗-待重試/推送失敗-已放棄）、推送重試次數、保留截止日期（報告日期+1天）
- **排程設定（Schedule Config）**: 代表系統的排程配置。包含屬性：執行時間（例如 08:00）、時區、是否啟用、上次執行時間
- **過濾規則（Filter Rule）**: 代表內容過濾的規則。包含屬性：相關性判斷方法（AI API 語義判斷）、相關主題清單（AI 模型、AI 工具、程式碼輔助、開發框架等）、AI 相關性比重計算方式（透過 AI API 分析內容中 AI 相關主題的段落占比，若 AI 相關段落超過 50% 則視為相關內容）、相關性門檻、內容長度限制、去重相似度門檻（預設 80%）、相似度演算法類型（Levenshtein 距離/Cosine 相似度）、內容指紋演算法（如 SimHash 或 MD5）
- **API 關鍵字配置（API Keywords Config）**: 代表用於技術新聞聚合 API 查詢的關鍵字。包含屬性：關鍵字清單（如「AI coding」、「large language model」、「GPT」、「Claude」、「LLM」等）、語言（預設 English）、排序方式（按時間/相關性）
- **來源配置檔案（Sources Config）**: 代表系統讀取的外部配置檔案結構。包含屬性：配置檔案格式（JSON/YAML）、來源清單（包含多個資訊來源配置）、全域設定（如預設關鍵字、預設時區）、配置版本號（用於追蹤配置變更）。每個來源配置包含：來源名稱、來源類型（API/RSS/Web）、來源層級（1/2/3）、URL 或 API 端點、環境變數名稱（用於讀取 API 金鑰）、搜尋關鍵字（選填）、是否啟用（enabled）、來源專屬參數（如請求頻率、超時時間等）
- **執行日誌（Execution Log）**: 代表單次系統執行的完整日誌記錄。包含屬性：執行開始時間戳、執行結束時間戳、總執行時間、各來源蒐集狀態清單（來源名稱、成功/失敗狀態、蒐集到的資訊數量、錯誤類型、錯誤訊息、執行時間）、總蒐集資訊數量、成功來源數量、失敗來源數量、摘要報告產生狀態、推送狀態、日誌檔案路徑

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: 系統能在設定的排程時間後 5 分鐘內完成蒐集與摘要產生流程（從排程時間開始計算，到摘要報告檔案產生完成為止）
- **SC-002**: 每次執行能成功從至少 3 個不同來源蒐集資訊，產生包含至少 5 則資訊的摘要報告
- **SC-003**: 摘要報告中至少 90% 的資訊與 AI 或 AI Coding 高度相關（透過人工抽樣驗證，樣本大小為每次報告中隨機抽取 10 則資訊，評估標準為內容是否包含 AI 模型、AI 工具、程式碼輔助、開發框架等相關主題）
- **SC-004**: 當識別到重複內容時，系統能在 95% 的情況下正確去重並保留內容最完整的版本（測試資料集包含至少 20 組已知重複內容，驗證方法為比對系統選擇的版本是否符合「內容最完整」判斷標準）
- **SC-005**: 針對英文內容產生的繁體中文摘要，使用者可讀性評分達到 4 分以上（5 分制，評分標準包括：語句通順度、術語翻譯準確度、資訊完整度，評估者需具備繁體中文母語能力及 AI 領域基礎知識）
- **SC-006**: 當單一來源無法連線時，系統仍能繼續運作並從其他來源蒐集資訊，不發生崩潰
- **SC-007**: 使用者能在摘要報告產生後 1 小時內收到推送通知或報告檔案
- **SC-008**: 使用者平均每天花費在瀏覽 AI 情報的時間從 30 分鐘減少至 10 分鐘以內（透過使用者回饋調查衡量，問卷設計包含使用前後時間對比、任務完成效率評估，樣本要求至少 5 位使用者連續使用 2 週後填寫）

## Assumptions

1. 使用者具備基本的系統配置能力，包括：(a) 能夠編輯 JSON 或 YAML 檔案，(b) 能夠設定環境變數，(c) 能夠設定系統排程任務（如 cron job 或 Windows Task Scheduler），(d) 能夠執行命令列指令
2. 系統運行環境具備穩定的網路連線，最低頻寬要求為 1 Mbps（下載），最大延遲不超過 1000ms，能存取公開的網路資源且無防火牆封鎖主要 AI 新聞網站
3. 主要資訊來源採用 API 優先策略，NewsAPI、Hacker News API、Reddit API 等提供穩定的 JSON 格式資料，RSS 標準（RSS 2.0、Atom）被官方部落格廣泛支援
4. 使用者主要關注英文資訊來源，但需要繁體中文摘要以提升閱讀效率
5. 初期版本的推送頻道以電子郵件為主，後續可擴展至其他通訊平台
6. MVP 階段採用三層級來源架構（官方部落格、新聞聚合 API、社群與電子報），Twitter 和 GitHub 整合延後至後續版本
7. 內容去重主要依賴標題相似度和關鍵摘要比對，不需要深度語義分析
14. 使用者願意註冊並取得必要的 API 金鑰（如 NewsAPI.org、Reddit API），這些服務多提供免費或低成本方案
15. 官方技術部落格（OpenAI、Anthropic、Google AI、DeepMind）持續提供公開可存取的 RSS feeds 或 API
16. 技術新聞聚合 API（如 NewsAPI）的關鍵字搜尋能有效過濾出 AI 相關內容，減少手動過濾負擔
8. 系統每日執行一次已足夠滿足使用者需求，不需要即時或多次執行
9. 使用者願意承擔外部 AI API（OpenAI 或 Anthropic）的持續使用成本
10. 外部 AI API 服務穩定可用，具備足夠的速率限制配額以處理每日摘要生成需求（假設可用性 SLA 達 99.9%，每日 API 配額足以處理至少 100 則資訊的摘要生成）。風險：若 API 服務中斷超過 1 小時，當日摘要將包含原始英文內容
11. 使用者具備本地機器或自有伺服器，能保持系統在排程時間運行（不需要 24/7 高可用性）
12. 使用者能夠設定和管理排程任務（如 cron job、Windows Task Scheduler 或類似機制）
13. 使用者了解如何設定環境變數，並能妥善保管敏感資訊（不將環境變數配置提交到版本控制系統）
17. 使用者具備基本的 JSON 或 YAML 語法知識，能正確編輯配置檔案
18. 配置檔案路徑固定或透過環境變數指定，系統無需實作複雜的配置檔案自動搜尋機制
19. 配置檔案的來源數量在合理範圍內（例如 10-50 個來源），不需要針對大規模配置（如數百個來源）進行效能優化
20. 使用者願意在新增來源時參考文件或範例配置，系統無需提供圖形化配置編輯介面
21. 外部依賴服務版本假設：(a) NewsAPI 使用 v2 API，(b) Hacker News API 使用 v0（無版本號但穩定），(c) Reddit API 使用 OAuth2 認證，(d) GitHub API 使用 v3 REST API 或 GraphQL v4。系統設計應具備 API 版本變更時的相容性處理能力

## Out of Scope (Not Included in This Feature)

1. 使用者帳號管理與多使用者支援（初期版本為單一使用者設計）
2. 自訂過濾規則的圖形化介面（初期透過設定檔配置）
3. 歷史摘要報告的查詢與搜尋功能
4. 行動裝置 App（僅提供桌面端或伺服器端運行）
5. 資訊來源的自動發現與推薦機制
6. AI API 成本優化機制（如快取、批次處理、模型選擇策略）
7. 雲端部署支援與高可用性架構（系統設計為本地運行，不提供雲端託管服務）
8. Twitter (X) 平台整合（MVP 階段不包含，延後至後續版本）
9. GitHub 趨勢專案蒐集與 octokit 整合（雖然推薦使用 octokit，但 MVP 階段優先完成其他來源）
10. 社群媒體熱度指標追蹤（按讚數、分享數、星星數等）- 僅保留社群平台的投票數/評分
11. 電子報內容的深度解析（MVP 階段僅抓取電子報的 Web 版連結，不進行複雜的 HTML 內容提取）
12. 多種程式語言技術棧支援（MVP 階段假設開發者使用 JavaScript/TypeScript 進行開發）
13. 配置檔案的圖形化編輯介面（使用者需直接編輯 JSON/YAML 文件）
14. 配置檔案的版本控制整合（如自動比對配置變更、回滾機制）
15. 配置檔案的動態熱更新（系統在每次排程執行時重新載入配置即可，不需要即時偵測檔案變更）
16. 配置檔案的多層級繼承或模板機制（例如基礎配置檔 + 環境特定配置檔）
17. 配置檔案的加密儲存（敏感資訊應透過環境變數管理，配置檔案僅包含非敏感資訊）
18. 配置驗證的進階規則引擎（例如跨欄位驗證、條件式驗證規則）
